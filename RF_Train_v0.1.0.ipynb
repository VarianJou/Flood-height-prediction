{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import Tabular training data",
   "id": "43cc1de66152eaef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "labels = pd.read_csv('FILEPATH')\n",
    "df = pd.read_csv('FILEPATH')"
   ],
   "id": "a95d2c39356deeb9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Prepare data for modeling",
   "id": "31b5c3b96749a386"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.merge(labels, df, on = 'ID')\n",
    "df = df.drop(columns=['ID'])\n",
    "df_wlabs = df.dropna()"
   ],
   "id": "c739988eec1f22c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X = df_wlabs.drop(['HWMdepth_m'] , axis= 1).values\n",
    "y =  df_wlabs['HWMdepth_m'].values\n",
    "\n",
    "plt.plot(X,y)\n",
    "\n",
    "plt.show()"
   ],
   "id": "d17e0ee35190c261"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]"
   ],
   "id": "745e6315b9b3cf99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# First split: 70% train, 30% temp (which will be split into 15% val + 15% test)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=54)\n\n# Second split: Split the 30% temp into 15% val and 15% test (50-50 split of the temp data)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=54)\n\nprint('Number of training observations:', y_train.shape[0], f'({y_train.shape[0]/len(y)*100:.1f}%)')\nprint('Number of validation observations:', y_val.shape[0], f'({y_val.shape[0]/len(y)*100:.1f}%)')\nprint('Number of test observations:', y_test.shape[0], f'({y_test.shape[0]/len(y)*100:.1f}%)')",
   "id": "99501be775a76b20"
  },
  {
   "cell_type": "code",
   "id": "2nnjq4l1mmy",
   "source": "# Z-score standardization based on training set statistics (prevents data leakage)\nX_mean = X_train.mean(axis=0)\nX_std = X_train.std(axis=0)\n\n# Apply z-score standardization: (X - mean) / std\nX_train_norm = (X_train - X_mean) / X_std\nX_val_norm = (X_val - X_mean) / X_std\nX_test_norm = (X_test - X_mean) / X_std\n\nprint('Training set standardized - shape:', X_train_norm.shape)\nprint('Validation set standardized - shape:', X_val_norm.shape)\nprint('Test set standardized - shape:', X_test_norm.shape)\nprint('\\nTraining set statistics used for standardization:')\nprint('Mean:', X_mean)\nprint('Std:', X_std)\nprint('\\nAfter z-score standardization:')\nprint('Training set - Mean: {:.6f}, Std: {:.6f}'.format(X_train_norm.mean(), X_train_norm.std()))\nprint('Validation set - Mean: {:.6f}, Std: {:.6f}'.format(X_val_norm.mean(), X_val_norm.std()))\nprint('Test set - Mean: {:.6f}, Std: {:.6f}'.format(X_test_norm.mean(), X_test_norm.std()))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hyperparameter Tuning and Training of the RF Model",
   "id": "b8e34df175187237"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Robust Hyperparameter Tuning with Cross-Validation\n",
    "rf = RandomForestRegressor(random_state=54)\n",
    "\n",
    "# Parameter grid for regression\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Cross-validation for regression\n",
    "rf_cv = RandomizedSearchCV(\n",
    "    rf, \n",
    "    rf_params, \n",
    "    n_iter=100,  # More iterations for better search\n",
    "    cv=5,       # 10-fold CV for more robust evaluation\n",
    "    scoring='neg_mean_squared_error',  # MSE for regression\n",
    "    random_state=54,\n",
    "    n_jobs=-1,   # Use all cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting hyperparameter optimization...\")\n",
    "rf_optimized = rf_cv.fit(X_train_norm, y_train)\n",
    "\n",
    "print('Best parameters:', rf_optimized.best_params_)\n",
    "print('Best cross-validation score (neg_MSE):', rf_optimized.best_score_)"
   ],
   "id": "c7ef6d80331a0f9"
  },
  {
   "cell_type": "code",
   "id": "vqo5uy177b",
   "source": "# Model Evaluation on All Sets\nbest_rf = rf_optimized.best_estimator_\n\n# Predictions on all sets\ny_train_pred = best_rf.predict(X_train_norm)\ny_val_pred = best_rf.predict(X_val_norm)\ny_test_pred = best_rf.predict(X_test_norm)\n\nprint(\"=\"*60)\nprint(\"MODEL PERFORMANCE METRICS\")\nprint(\"=\"*60)\n\ndef calculate_regression_metrics(y_true, y_pred, set_name):\n    mse = metrics.mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    mae = metrics.mean_absolute_error(y_true, y_pred)\n    r2 = metrics.r2_score(y_true, y_pred)\n    \n    print(f\"\\n{set_name} Set Metrics:\")\n    print(\"-\" * 30)\n    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n    print(f\"R² Score: {r2:.4f}\")\n    \n    return {\n        'mse': mse,\n        'rmse': rmse,\n        'mae': mae,\n        'r2': r2\n    }\n\n# Calculate metrics for all sets\ntrain_metrics = calculate_regression_metrics(y_train, y_train_pred, \"Training\")\nval_metrics = calculate_regression_metrics(y_val, y_val_pred, \"Validation\")\ntest_metrics = calculate_regression_metrics(y_test, y_test_pred, \"Test\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "wxvkneodmm7",
   "source": "# Regression Visualization: Actual vs Predicted\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\ndef plot_actual_vs_predicted(y_true, y_pred, ax, title, metrics):\n    ax.scatter(y_true, y_pred, alpha=0.6)\n    \n    # Perfect prediction line\n    min_val = min(min(y_true), min(y_pred))\n    max_val = max(max(y_true), max(y_pred))\n    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n    \n    ax.set_xlabel('Actual Values')\n    ax.set_ylabel('Predicted Values')\n    ax.set_title(f'{title} Set\\nR² = {metrics[\"r2\"]:.3f}, RMSE = {metrics[\"rmse\"]:.3f}')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplot_actual_vs_predicted(y_train, y_train_pred, axes[0], 'Training', train_metrics)\nplot_actual_vs_predicted(y_val, y_val_pred, axes[1], 'Validation', val_metrics)\nplot_actual_vs_predicted(y_test, y_test_pred, axes[2], 'Test', test_metrics)\n\nplt.tight_layout()\nplt.show()\n\n# Residual plots\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\ndef plot_residuals(y_true, y_pred, ax, title):\n    residuals = y_true - y_pred\n    ax.scatter(y_pred, residuals, alpha=0.6)\n    ax.axhline(y=0, color='r', linestyle='--')\n    ax.set_xlabel('Predicted Values')\n    ax.set_ylabel('Residuals')\n    ax.set_title(f'{title} Set - Residual Plot')\n    ax.grid(True, alpha=0.3)\n\nplot_residuals(y_train, y_train_pred, axes[0], 'Training')\nplot_residuals(y_val, y_val_pred, axes[1], 'Validation')\nplot_residuals(y_test, y_test_pred, axes[2], 'Test')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9txqc7dr68",
   "source": [
    "# Feature Importance Analysis\n",
    "feature_importance = best_rf.feature_importances_\n",
    "\n",
    "# Get feature names (assuming they match the original dataframe columns)\n",
    "try:\n",
    "    feature_names = df_wlabs.drop(['HWMdepth_m'], axis=1).columns\n",
    "except:\n",
    "    # If feature names not available, create generic names\n",
    "    feature_names = [f'Feature_{i}' for i in range(len(feature_importance))]\n",
    "\n",
    "# Create feature importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE RANKINGS\")\n",
    "print(\"=\"*60)\n",
    "print(importance_df.to_string(index=False))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "# Horizontal bar plot for top 20 features\n",
    "top_features = importance_df.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Feature Importance (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Add importance values on bars\n",
    "for i, v in enumerate(top_features['importance']):\n",
    "    plt.text(v + 0.001, i, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Cumulative importance plot\n",
    "cumulative_importance = np.cumsum(importance_df['importance'].values)\n",
    "plt.plot(range(1, len(cumulative_importance) + 1), cumulative_importance, 'b-', linewidth=2)\n",
    "plt.axhline(y=0.8, color='r', linestyle='--', label='80% of importance')\n",
    "plt.axhline(y=0.9, color='orange', linestyle='--', label='90% of importance')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Cumulative Importance')\n",
    "plt.title('Cumulative Feature Importance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find number of features needed for 80% and 90% of importance\n",
    "idx_80 = np.argmax(cumulative_importance >= 0.8) + 1\n",
    "idx_90 = np.argmax(cumulative_importance >= 0.9) + 1\n",
    "\n",
    "print(f\"\\nFeature Selection Insights:\")\n",
    "print(f\"Number of features needed for 80% of importance: {idx_80}\")\n",
    "print(f\"Number of features needed for 90% of importance: {idx_90}\")\n",
    "print(f\"Total number of features: {len(feature_importance)}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jzjhzy6yj0j",
   "source": "# Final Model Summary\nprint(\"=\"*80)\nprint(\"FINAL MODEL SUMMARY\")\nprint(\"=\"*80)\nprint(f\"Best Random Forest Parameters: {rf_optimized.best_params_}\")\nprint(f\"Best Cross-Validation Score (neg_MSE): {rf_optimized.best_score_:.4f}\")\nprint(f\"\\nFinal Test Set Performance:\")\nprint(f\"  R² Score: {test_metrics['r2']:.4f}\")\nprint(f\"  RMSE: {test_metrics['rmse']:.4f}\")\nprint(f\"  MAE: {test_metrics['mae']:.4f}\")\nprint(f\"  MSE: {test_metrics['mse']:.4f}\")\n\n# Check for overfitting\nprint(f\"\\nOverfitting Check:\")\nprint(f\"  Training R²: {train_metrics['r2']:.4f}\")\nprint(f\"  Validation R²: {val_metrics['r2']:.4f}\")\nprint(f\"  Test R²: {test_metrics['r2']:.4f}\")\nprint(f\"  Train-Val R² Gap: {train_metrics['r2'] - val_metrics['r2']:.4f}\")\nprint(f\"  Train-Test R² Gap: {train_metrics['r2'] - test_metrics['r2']:.4f}\")\n\nif train_metrics['r2'] - test_metrics['r2'] > 0.2:\n    print(\"  WARNING: Potential overfitting detected (>20% R² gap between train and test)\")\nelse:\n    print(\"  Model shows good generalization\")\n\nprint(\"\\n\" + \"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
